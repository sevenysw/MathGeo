{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500495b6-ff3b-4296-8c7f-b66fe9f93353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ba332b-0847-4c56-8272-79eb5a0bbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9873cac4-2cb7-4ad1-abd3-f97a10b0f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6eab72-2f5d-47c6-a1ac-50b0deb4c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) / (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc38298b-dccb-45a4-aa93-3be3093a3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b951aaec-f78d-43b5-b971-71573369123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acd3eb6-1756-47fb-8fd2-4272c33ee054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6595289-acbd-4d53-8347-e2b469a2dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcd9737-1831-499f-82b4-36fc7108ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ca821a-0590-4d77-9fe8-de57cc5c7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d4389a-a8c6-4dec-a9e1-2a6442982178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1786a7-8058-4936-a051-eff00f7db6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ir = datasetting.matload(\"x_ir_1.mat\",\"x_ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5163f9-2439-4858-9083-013bb7b12924",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datasetting.matload(\"y_1.mat\",\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169f340-6607-4b08-a70b-873a53034ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = datasetting.matload(\"m_1.mat\",\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8043c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = datasetting.matload(\"mt_1.mat\",\"mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8fc268a-aa60-496a-9bce-757c4ba28bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ir = torch.Tensor(x_ir)\n",
    "y = torch.Tensor(y)\n",
    "m = torch.Tensor(m)\n",
    "mt = torch.Tensor(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0341c713-d7a8-419b-a1ef-f3e1607de4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a938cc3-e574-4d26-ab3d-0aafce840552",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ir = x_ir.to(device)\n",
    "y = y.to(device)\n",
    "m = m.to(device)\n",
    "mt = mt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6999788-6c6b-412a-8874-54df84590c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ir = x_ir.view(-1,1,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0efd060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61a94866-93c6-46f0-b297-6dca70283f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view(-1,1,64,64)\n",
    "m = m.view(-1,1,64,32)\n",
    "mt = mt.view(-1,1,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fd4159b-4367-48ef-ab48-09eae9751dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import datasetting\n",
    "\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02cf8978-e203-4828-b309-24432674f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "546701b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a54e56d-2544-4e34-9476-f4ef5ff29348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (init_conv): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPositionEmbeddings()\n",
       "    (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (2): GELU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
       "        (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightStandardizedConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mid_block1): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (mid_attn): Residual(\n",
       "    (fn): PreNorm(\n",
       "      (fn): Attention(\n",
       "        (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (norm): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (mid_block2): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (final_res_block): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightStandardizedConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightStandardizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet(\n",
    "    dim=64,\n",
    "    channels=2,\n",
    "    dim_mults=(1, 2, 4,8),\n",
    "    out_dim=1\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1702c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd9e62d4-14d6-4e7e-8e9f-12ba8fc5d498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('net_params_500.pkl' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5df3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpolator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73b76544-d585-4d4a-9fa6-593e342ac3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x, x_dis, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(torch.cat((x, x_dis),1) , t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    \n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, x_ir, m, mt, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "    imgs_dis = []\n",
    "    pos_reg = torch.arange(1,65).view(1,64)\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        x_ir_rec = torch.matmul(mt,img)\n",
    "        x_dis = torch.matmul(m,x_ir_rec-x_ir)\n",
    "        img = p_sample(model, img, x_dis, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "        imgs_dis.append(x_dis.cpu().numpy())\n",
    "    return imgs,imgs_dis\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x_ir, m, mt, image_size=64, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, x_ir, m, mt, shape=(batch_size, channels, image_size, image_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f692fb0-3e2b-4646-8ef9-d59a154606d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274b61ea27e47d7ade9be6cf836a244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample 64 images\n",
    "samples,samples_dis = sample(model, x_ir, m, mt, image_size=64, batch_size=1, channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6367af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "samples_np = np.zeros((1000,64,64))\n",
    "samples_dis_np = np.zeros((1000,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44ce43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    samples_np[i,:,:] = samples[i][0][0]\n",
    "    samples_dis_np[i,:,:] = samples_dis[i][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(\"samples1.mat\",{\"samples\":samples_np})\n",
    "sio.savemat(\"samples_dis1.mat\",{\"samples_dis\":samples_dis_np})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
